[
["index.html", "Corona-Morphologie: Einfache Produktivitätsanalysen an konkreten Beispielen 1 Einführung", " Corona-Morphologie: Einfache Produktivitätsanalysen an konkreten Beispielen Stefan Hartmann 2020-06-08 1 Einführung Eine zentrale Frage insbesondere in der Wortbildungsforschung, aber auch in anderen Bereichen wie etwa der Syntax, lautet, in welchem Maße ein konkretes Muster in der Lage ist, Neubildungen hervorzubringen. Beispielsweise bringt das Suffix -icht (Dickicht, Kehricht) synchron gar keine Neubildungen mehr hervor, während etwa -mäßig relativ viele Neubildungen wie etwa rezomäßig hervorbringt. In diesem Tutorial kann ich zwar nicht auf komplexere Aspekte der Produktivitätsmessung eingehen, möchte aber zumindest einen groben Überblick über verbreitete Maße der morphologischen Produktivität geben und anhand eines konkreten Beispiels zeigen, wie sie sich operationalisieren lassen. Anschließend zeige ich noch, wie man mit Hilfe moderner Visualisierungsverfahren die Entstehung neuer Wörter quasi in Echtzeit verfolgen kann. Wenn Sie die konkreten Anwendungsbeispiele selbst ausprobieren möchten, brauchen Sie für Kapitel 2 und 3 nur Excel oder ein anderes Tabellenkalkulationsprogramm wie LibreOffice Calc. Das vierte und fünfte Kapitel verwendet R und ist vermutlich eher für Fortgeschrittene interessant, die sich schon ein wenig mit R oder einer anderen Programmiersprache wie z.B. Python auskennen - alle anderen können versuchen, den konzeptionellen Ausführungen zu folgen, und/oder sich einfach die bunten Grafiken anschauen… "],
["produktivitatsmae.html", "2 Produktivitätsmaße", " 2 Produktivitätsmaße Um die Produktivität sprachlicher Muster zu messen, bedient man sich i.d.R. dreier Größen, die auch in anderen Bereichen relevant sind. Für die gängigen Produktivitätsmaße benötigt man nämlich die Anzahl der Tokens die Anzahl der Types die Anzahl der Hapax Legomena. Was hat es damit auf sich? Tokens bezieht sich die Gesamtzahl der Wörter, die zu einem Wortbildungsmuster gehören. Angenommen, wir interessieren uns für Derivate auf -heit und -keit wie Freiheit und Ehrlichkeit und finden in einem winzig kleinen Korpus die Belege Freiheit, Freiheit, Freiheit, Ehrlichkeit, Faulheit und nochmal Faulheit, dann sind das insgesamt sechs Tokens, also sechs Vorkommnisse. Types bezeichnet die Anzahl der unterschiedlichen Instanzen eines Wortbildungsmusters. Im eben genannten Beispiel wären dies die drei Lexeme Freiheit, Ehrlichkeit und Faulheit. Hapax Legomena bezeichnet diejenigen Instanzen, die genau ein einziges Mal belegt sind. In unserem Beispiel wäre das Ehrlichkeit. Hapax Legomena, oder kurz Hapaxe, sind deshalb interessant, weil sie häufig als Index für Neubildungen gesehen werden: Zwar ist nicht davon auszugehen, dass jedes Wortbildungsprodukt, das nur einmal in einem Korpus belegt ist, eine ad-hoc-Bildung ist, denn auch eigentlich relativ häufige Wörter können in einem Korpus durch Zufall unterrepräsentiert sein. Man geht aber dennoch davon aus, dass es eine Korrelation zwischen Neubildungen und Hapaxen gibt, sodass Hapaxe als ungefährer Richtwert für die Zahl an Neubildungen gesehen werden können (die sich ja nicht messen lässt). An einem nicht-linguistischen Beispiel (das eine Idee von Susanne Flach aufgreift) illustriert 2.1 diese drei Konzepte. Gerade die Unterscheidung von Types und Tokens ist anhand solcher nicht-linguistischer Beispiele oft besser nachzuvollziehen. Zugleich zeigen solche Beispiele, dass das Konzept von Types immer von der Abstraktionsebene abhängt, auf der man sich gerade bewegt: Wenn es uns um die unterschiedlichen Arten von Cocktails geht, dann sehen wir dort drei Types - wenn es uns um die unterschiedlichen Arten von Getränken geht (“Cocktail” vs. “Bier” vs. “Limonade” etc.), dann sehen wir nur einen Type - nämlich Cocktails! Dementsprchend kann man auch bei der Arbeit mit linguistischen Daten Types oft auf unterschiedliche Art und Weise bestimmen (z.B. rein graphematisch, unter Zuhilfenahme von Grundformen [Lemmas] usw.). Fig. 2.1: Types und Tokens Mit Hilfe dieser drei Maße können wir uns nun den Produktivitätsmaßen annähern, die Baayen (u.a. 1993, 2009) in den 90er-Jahren vorgeschlagen hat und die seitdem immer wieder angewandt, aber auch kontrovers diskutiert wurden, da jedes dieser Maße nur einen Teilaspekt der morphologischen Produktivität erfassen kann und die Maße teilweise auch zu Fehlinterpretationen einladen, wenn man sich nicht ihrer Grenzen bewusst ist. Die realisierte Produktivität ist schlicht die Typefrequenz, die i.d.R. normalisiert berichtet wird, indem man die Anzahl der Tokens durch die Anzahl der Types teilt. Das wohl am häufigsten verwendete Maß ist die potentielle Produktivität. Sie ist der Quotient aus der Anzahl der Hapax Legomena, die zu einem Wortbildungsmuster gehören, und der Anzahl der Tokens im Korpus insgesamt. Dieses Maß wurde beispielsweise verwendet, um die Produktivität unterschiedlicher Wortbildungsmuster synchron zu vergleichen (Baayen &amp; Lieber 1991) oder um die Produktivität desselben Wortbildungsmusters in unterschiedlichen Zeitspannen zu vergleichen (Scherer 2006, Hartmann 2016). Dabei ist jedoch zu bedenken, dass der Wert der potentiellen Produktivität von der Korpusgröße und auch von der Tokenfrequenz abhängt (vgl. z.B. Gaeta &amp; Ricca 2006). Produktivitätswerte, die auf Grundlage unterschiedlich großer (Teil-)Korpora gewonnen wurden oder bei denen sich die Tokenfrequenz des Wortbildungsmusters zwischen den beiden (Teil-)Korpora unterscheiden, sind daher nicht unmittelbar vergleichbar - ein Problem, das leider oft nicht berücksichtigt wird (und das auch ich lange unterschätzt habe, vgl. Hartmann 2016; erst in Hartmann 2018 habe ich dem Rechnung getragen). Die expandierende Produktivität schließlich versucht zu messen, wie stark ein Wortbildungsmuster zum Wachstum des Wortschatzes einer Sprache beiträgt, indem sie die Anzahl der Hapaxe, die zum Wortbildungsmuster gehören, durch die Anzahl aller Hapaxe im Korpus teilt. Im Folgenden werden wir uns nur mit den ersten beiden Maßen auseinandersetzen und sie an einem konkreten Beispiel errechnen. "],
["corona-als-erstglied-eine-produktivitatsanalyse.html", "3 Corona- als Erstglied: Eine Produktivitätsanalyse Produktivitätsanalyse in Excel", " 3 Corona- als Erstglied: Eine Produktivitätsanalyse Als Beispiel sehen wir uns Komposita mit dem Erstglied Corona- an. Hier kann man zwar streiten, ob es sich um ein eigenes Wortbildungsmuster handelt oder ob wir es einfach mit Instanzen der allgemeineren N+N-Komposition zu tun haben, allerdings lässt sich durchaus argumentieren, dass Corona-Komposita in letzter Zeit so häufig geworden sind, dass man [Corona+X] als eigenständiges Kompositionsmuster annehmen kann. Dabei kann es nicht nur aus morphologischer, sondern auch aus sozio- und diskurslinguistischer Sicht interessant sein, einen Blick darauf zu werfen, mit welchen Bestimmungsgliedern sich das Erstglied Corona- verbindet. Praktischerweise können wir über das Digitale Wörterbuch der Deutschen Sprache (DWDS) mittlerweile auf ein Corona-Korpus zugreifen (Barbaresi 2020), das nach einmaliger Registrierung zugänglich ist. Aufgrund der flexibleren Suchanfragemöglichkeiten verwenden wir das Tool Dstar (https://kaskade.dwds.de/dstar/), um das Korpus zu durchsuchen. (Eine gute Anleitung zu Dstar findet sich in diesem Tutorial von Andreas Blombach: http://sprachwissenschaft.fau.de/personen/daten/blombach/korpora.pdf) Mit folgender Suchanfrage lasse ich mir alle Instanzen auszählen, bei denen auf die Buchstabenfolge Corona, ggf. gefolgt von einem Bindestrich, noch mindestens ein weiterer Buchstabe folgt: count( $w=/Corona-?.+/gi ) #by[$l]. (Das “gi” nach dem regulären Ausdruck ist eine Eigenheit der DWDS-Suchanfragesprache DDC: Mit g gebe ich an, dass ich genau das suche, also bspw. nichts, wo dem String Corona noch Anderes vorausgeht, und mit i gebe ich an, dass die Suche case-insensitive sein soll, also Groß- und Kleinschreibung ignoriert werden sollen). Fig. 3.1: Types und Tokens Wie 3.1 zeigt, habe ich als Exportoption “Text” gewählt, sodass wir die Daten im Rohtextformat erhalten, und die Seitengröße (“Page size”) so weit erhöht, dass auf jeden Fall alle Types ausgegeben werden (der Default wäre hier bei weitem nicht ausreichend). So erhalten wir eine Tabelle mit einzelnen Types und deren jeweiliger Frequenz. Mit dieser arbeiten wir nun weiter. Die Tabelle können Sie, wenn Sie sich nicht bei DWDS anmelden und die Recherche selbst durchführen wollen, auch ganz einach hier herunterladen. Produktivitätsanalyse in Excel Die Tabelle können wir z.B. in Excel copy&amp;pasten (dabei können Sie sich ggf. an folgendem Tutorial orientieren: https://empirical-linguistics.github.io/korpus-schnelleinstieg/von-der-fragestellung-zur-konkordanz.html#import-in-excel) In 3.2 habe ich die Tabelle zudem noch mit Überschriften versehen, die in der von Dstar erzeugten Datei zunächst nicht dabei sind, und habe sie als Tabelle formatiert (Einfügen &gt; Tabelle, vgl. wiederum das oben verlinkte Tutorial). Fig. 3.2: Types und Tokens Es ist nun denkbar einfach, die Zahl der Types, Tokens und Hapax Legomena zu errechnen. Wenn man es sich ganz einfach machen möchte, kann man die Types ermittelt, indem man schaut, wie viele Zeilen die Tabelle hat (dabei muss man die Kopfzeile abziehen) und die Anzahl der Tokens, indem man mit Excels AutoSum-Funktion die Frequenzen in der linken Spalte aufsummiert. Die Hapax Legomena kann man zählen, indem man die Daten nach Frequenz ordnet und dann mit Hilfe der Zeilennummerierung errechnet, wie viele Types genau einmal belegt sind. Man kann es aber auch etwas professioneller machen, indem man die Frequenzwerte in der “Freq”-Spalte mit Hilfe einer PivotTable auszählt. Dafür klickt man auf Einfügen &gt; PivotTable und setzt im sich öffnenden neuen Fenster ein Häkchen bei “Freq”. Defaultmäßig zeigt Excel nun die Summe von “Freq”. Dieser Wert ist die Gesamtzahl der Tokens: 91.606. Um stattdessen die Zahl der Types und Hapaxe zu bekommen, ändern wir im “Werte”-Fenster unten rechts die Funktion, die die Daten auswertet, von “Summe” zu “Zählen” (engl. count). Nun sehen wir zunächst die Anzahl der Types (3976). Wenn wir nun “Freq” aus dem Fenster oben ins Fenster “Zeilen” unten links ziehen, dann wird das Ganze nach den einzelnen Frequenzwerten aufgeschlüsselt: Nun sehen wir also, wie viele Types 1-mal belegt sind, wie viele 2-mal usw. Daraus können wir die Anzahl der Hapax Legomena ablesen: 2313. Wir haben nun also alle Werte, die wir brauchen: Anzahl der Tokens: 91.606 Anzahl der Types: 3976 Anzahl der Hapax Legomena: 2313 Theoretisch könnte man in Excel nun direkt die einzelnen Werte berechnen. Um in Excel etwas zu berechnen, muss man ein Gleichheitszeichen davor setzen; die Formeln wären also: Potentielle Produktivität: Anzahl der Hapaxe / Anzahl der Tokens: = 2313 / 91606 Realisierte Produktivität: Zahl der Types / Zahl der Tokens: = 3976 / 91606 Da wir die Anzahl der Hapaxe nicht kennen, verzichten wir darauf, die expandierende Produktivität zu ermitteln. Allerdings ließe sich die Gesamtzahl der Hapax Legomena im Coronakorpus relativ einfach über Dstar herausfinden, indem man mit count(* #sep) #by[$w] alle Tokens im Coronakorpus auszählen lässt und dann die Hapaxe so auszählt, wie wir es eben getan haben - allerdings ist die Datenmenge sehr viel größer, sodass diese Aufgabe einfacher zu erledigen ist, wenn man über basale Programmierkenntnisse verfügt und eine Programmiersprache wie R oder Python diese Aufgabe erledigen lassen kann. Bevor wir weitermachen, müssen wir uns noch kurz der Frage widmen, was uns die hier errechneten Produktivitätsmaße eigentlich sagen. Die nüchterne Antwort ist: Zunächst einmal nichts. Solche Werte machen nur im Kontext Sinn - also wenn man beispielsweise, wie es die oben angeführte Forschungsliteratur tut, verschiedene Wortbildungsmuster miteinander vergleicht oder die Produktivitätsentwicklung diachron verfolgt (mit dem erwähnten Caveat, dass man eigentlich gleich große Datenmengen bräuchte). Im Falle unseres Beispiels könnte man bspw. die Produktivität des Musters 2019, 2020 und 2021 vergleichen (natürlich erst, wenn man die Daten dafür hat - ich schreibe dies Mitte 2020), oder man könnte die Produktivität von Corona-Komposita mit anderen Kompositionsmustern vergleichen, die sich auf ähnlich krisenhafte Ereignisse beziehen. Fig. 3.3: Zahl der Types und Hapaxe mit Hillfe von PivotTables in Excel ermitteln "],
["morphologische-produktivitat-live-ein-blick-auf-die-bestimmungsglieder.html", "4 Morphologische Produktivität live: Ein Blick auf die Bestimmungsglieder", " 4 Morphologische Produktivität live: Ein Blick auf die Bestimmungsglieder In diesem Abschnitt wollen wir uns nun einen anderen Weg anschauen, auf dem wir uns den Daten nähern können. Hierfür arbeiten wir mit R - falls Sie sich hier einarbeiten wollen, finden Sie online extrem viele gute Tutorials, und es gibt auch viele Einführungen in die Statistik speziell für Linguist*innen, die mit R arbeiten. Hier kann ich nicht in R einführen, versuche aber, das, was ich mache, so verständlich zu beschreiben, dass Sie zumindest das Konzept hinter dem, was ich mache, verstehen können. Zunächst lade ich einige Zusatzpakete, die ich im Folgenden benutzen möchte. Sie enthalten Funktionen, die in R nicht standardmäßig enthalten sind. Wenn Sie die Pakete noch nicht installiert haben, müssen Sie siejedes einzelne zunächst mit install.packages(&quot;Paketname&quot;) installieren. Das gilt für diejenigen, die über das “Comprehensive R Architecture Network” (CRAN) verfügbar sind. Bei einem der Pakete, “collostructions”, ist dies nicht der Fall, Sie finden es unter www.sfla.ch. library(tidyverse) library(googleVis) library(lubridate) library(collostructions) Zunächst lesen wir die Daten ein, werfen einen Blick auf die Struktur der Daten und schauen uns diese mit Hilfe des str()-Befehls genauer an. d &lt;- read_delim(&quot;Coronakomposita.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;Freq&quot;, &quot;word&quot;, &quot;date&quot;)) ## Parsed with column specification: ## cols( ## Freq = col_double(), ## word = col_character(), ## date = col_date(format = &quot;&quot;) ## ) # Hier lassen wir uns einfach die ersten paar Datenpunkte anzeigen: d ## # A tibble: 14,682 x 3 ## Freq word date ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 Corona 2020-03-04 ## 2 1 Corona* 2020-04-01 ## 3 1 Corona* 2020-04-03 ## 4 1 Corona* 2020-04-08 ## 5 1 Corona* 2020-04-09 ## 6 1 Corona* 2020-04-13 ## 7 1 Corona* 2020-04-18 ## 8 1 Corona* 2020-05-08 ## 9 1 Corona* 2020-05-18 ## 10 1 Corona* 2020-05-22 ## # … with 14,672 more rows str(d) ## tibble [14,682 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Freq: num [1:14682] 1 1 1 1 1 1 1 1 1 1 ... ## $ word: chr [1:14682] &quot;Corona&quot; &quot;Corona*&quot; &quot;Corona*&quot; &quot;Corona*&quot; ... ## $ date: Date[1:14682], format: &quot;2020-03-04&quot; &quot;2020-04-01&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Freq = col_double(), ## .. word = col_character(), ## .. date = col_date(format = &quot;&quot;) ## .. ) Nun löschen wir alle Datenpunkte, in denen “Corona” nicht von mindestens zwei weiteren Zeichen gefolgt wird, um Fehltreffer wie jene zu tilgen, die Ihnen sicherlich ins Auge gefallen sind, als wir oben einen Blick auf die ersten paar Datenpunkte geworfen haben. Dann konvertieren wir alle Daten in Kleinschreibung und fügen eine Spalte hinzu, die nur die Zweitglieder enthält (also alles, was auf Corona- folgt). Aus dieser Spalte löschen wir zusätzlich noch alle Interpunktionszeichen, um z.B. Fälle der Kompositaschreibung mit und ohne Bindestrich zu vereinheitlichen. # Fehltreffer reduzieren d &lt;- d[-which(sapply(1:nrow(d), function(i) nchar(d$word[i]))&lt;=8),] # Groß- und Kleinschreibung ignorieren d$word &lt;- tolower(d$word) # Köpfe d$head &lt;- gsub(&quot;corona-?&quot;, &quot;&quot;, d$word) # Interpunktion löschen d$head &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, d$head) Wir fassen die Daten nun mit der summarise-Funktion aus dem dplyr-Paket (Teil des “Tidyverse”) zusammen, um die Gesamtfrequenzen jedes Bestimmungsglieds zu bekommen, und ordnen die Daten absteigend nach Frequenz. Das Ganze speichern wir in einen eigene Dataframe, den wir corona_tbl nennen. corona_tbl &lt;- d %&gt;% group_by(head) %&gt;% summarise( Freq = sum(Freq) ) %&gt;% arrange(desc(Freq)) Mit Hilfe dieser Daten können wir nun sehr einfach auch die Kennzahlen errechnen, die wir auf der letzten Seite kennengelernt und mit Excel errechnet haben: # Anzahl der Types = Anzahl der Zeilen der eben genereierten Tabelle: types &lt;- nrow(corona_tbl) # Anzahl der Tokens = Summe der Frequenz aller Tokens: tokens &lt;- sum(corona_tbl$Freq) # Anzahl der Hapax Legomena: Anzahl der Types mit dem Frequenzwert 1: hapaxes &lt;- length(which(corona_tbl$Freq==1)) # Realisierte Produktivität: hapaxes / tokens ## [1] 0.01674374 # Potentielle Produktivität: types / tokens ## [1] 0.0302768 Wir können mit R aber noch viel mehr machen. So können wir mit Hilfe von R ein sogenanntes Motion Chart erstellen. In so einem Motion Chart könnnen wir dann die Entstehung neuer Wortbildungsprodukte quasi “live” mitverfolgen. Dafür brauchen wir aber zunächst eine Tabelle, die auch die date-Werte, also die Kalenderdaten, mit einschließt, die wir aus der soeben erstellten corona_tbl-Tabelle gelöscht haben. Wir verwenden wiede die summarise-Funktion, die wir oben bereits kennengelernt haben. Der Zusatz as_tibble ist hier aus technischen Gründen nötig, weil die GoogleVis-Funktion, die wir später benutzen, den entstehenden Dataframe sonst nicht als Input akzeptiert. “Tibble” ist das Default-Dataframe-Format der Tidyverse-Pakete, und der Output der summarise-Funktion ist zwar selbst ein(e?) Tibble, aber mit zusätzlichen Attributen, derer wir uns auf diese Weise entledigen. d2 &lt;- d %&gt;% group_by(head, date) %&gt;% summarise( Freq = sum(Freq) ) %&gt;% as_tibble Es ist in vielen Fällen sinnvoll, Frequenzen zu logarithmieren (vgl. hierzu z.B. https://de.wikipedia.org/wiki/Logarithmus). Gerade bei Frequenzdaten ist es üblich, sie zu logarithmieren, da Wortfrequenzen oft einer sehr schiefen Verteilung folgen, wie wir auch an diesen Daten sehen können: plot(corona_tbl$Freq, type = &quot;b&quot;, ylab = &quot;Absolute Frequenz&quot;) Einige wenige Tokens sind extremst häufig, viele sehr selten und die allermeisten Hapaxe, haben also eine Frequenz von 1. Anders sieht es aus, wenn wir die Daten logarithmieren: plot(log(corona_tbl$Freq), type = &quot;b&quot;, ylab = &quot;Logarithmierte Frequenz&quot;) Die Verteilung ist nun schon deutlich weniger schief. Fügen wir deshalb unserer Tabelle, die die Frequenz jedes einzelnen Bestimmungsglied für jedes einzelne Datum erfasst, noch eine Spalte mit logarithmierter Frequenz hinzu: d2$logFreq &lt;- log(d2$Freq) Nun können wir schon zur Visualisierung übergehen, da wir die relevanten Daten haben: Bestimmungsglied (head), Kalendertag (date) und (logarithmierte) Frequenz (logFreq). In unserem MotionChart wollen wir die Frequenz auf der y-Achse (also der vertikalen Achse) darstellen. Aber um die Daten im Raum anzuordnen, brauchen wir noch eine Information, wo sie sich horizontal, also auf der x-Achse, befinden sollen. Dafür fügen wir eine (bedeutunglose) Indexvariable mit randomisierten Zahlen ein. Mit Hilfe des set.seed-Befehls stelle ich im folgenden Code sicher, dass Sie “zufällig” genau das gleiche Sample erhalten werden wie ich. set.seed(100) # Dataframe mit Zufallszahl für jedes Lemma spl &lt;- tibble( head = unique(d2$head), index = sample(1:(length(unique(d2$head))), length(unique(d2$head))) ) # beide Dataframes verbinden d2 &lt;- left_join(d2, spl, by = &quot;head&quot;) Nun können wir uns schon an eine erste Visualisierung wagen. Weil die Daten allerdings viele Datenpunkte enthalten, die von 2019 stammen - die zum einen für uns nicht so interessant sind und bei denen zum anderen die Datierung teilweise zweifelhaft ist -, beschränken wir die Daten zunächst auf diejenigen aus dem Jahr 2020, wofür wir eine eigene “year”-Variable erstellen. d2$year &lt;- gsub(&quot;-.*&quot;, &quot;&quot;, d2$date) d2 &lt;- filter(d2, year == &quot;2020&quot;) Nun können wir die Daten mit Hilfe eines MotionChart visualisieren: bubble &lt;- gvisMotionChart(d2, &quot;head&quot;, &quot;date&quot;, xvar = &quot;index&quot;, yvar = &quot;logFreq&quot;, sizevar = &quot;logFreq&quot;, colorvar = &quot;&quot;) plot(bubble) Fig. 4.1: GoogleVis Motion Chart mit Frequenzen der Bestimmungsglieder, die im Coronakorpus mit dem Erstglied Corona- auftreten. "],
["assoziationsmuster.html", "5 Assoziationsmuster", " 5 Assoziationsmuster Wir möchten nun zusätzlich wissen, welche Lexeme überzufällig häufig als Bestimmungsglieder in Corona-Komposita auftreten. Dafür brauchen wir zunächst ein Referenzkorpus. Hierfür habe ich eine Frequenzliste aller als Nomen getaggten Tokens aus dem DWDS-Kernkorpus des 21. Jahrhunderts erstellt (wiederum mit Dstar), die wir nun einlesen. Wiederum konvertieren wir alle Daten in Kleinschreibung und zählen die Frequenz entsprechend aus: # einlesen library(tidyverse) nouns &lt;- read_delim(&quot;allnouns_dwds21.txt&quot;, delim = &quot;\\t&quot;, quote=&quot;&quot;, col_names = c(&quot;Freq_dwds21&quot;, &quot;word&quot;)) # alle in Kleinschreibung nouns$word &lt;- tolower(nouns$word) # neu auszählen (um groß- und kleingeschriebene Varianten zu vereinen) nouns &lt;- nouns %&gt;% group_by(word) %&gt;% summarise( Freq_dwds21 = sum(Freq_dwds21) ) Nun verbinden wir mit Hilfe der left_join-Funktion die weiter oben erstellte corona_tbl-Tabelle mit der soeben erstellten nouns-Tabelle. Da nicht alle Lexeme, die in der Corona-Tabelle belegt sind, auch in der DWDS21-Tabelle belegt sind, gibt es einige fehlende Datenpunkte (in R heißen diese NA, für “Not Available”). Diese ersetzen wir mit Hilfe des replace_na-Befehls durch 0. corona_tbl &lt;- left_join(corona_tbl, nouns, by = c(&quot;head&quot; = &quot;word&quot;)) corona_tbl &lt;- replace_na(corona_tbl, list(Freq_dwds21 = 0)) Mit Hilfe der Funktion collex.dist aus dem “collostructions”-Paket können wir nun eine sog. distinktive Kollexemanalyse über die Daten laufen lassen. Wenn Sie mehr über dieses Verfahren lesen möchten, können Sie z.B. das Paper von Gries &amp; Stefanowitsch (2004) oder den Überblicksartikel zur Kollostruktionsanalyse von Stefanowitsch (2013) lesen. Empfehlenswert sind auch die Einführungsvideos von Susanne Flach (sfla.ch), wo Sie auch eine Einführung in das gleichnamige R-Paket finden. Beachten Sie aber bitte, dass ich dieses Verfahren hier überhaupt nicht im Sinne der Erfinder verwende, sondern lediglich explorativ auf Datensätze anwende. Deshalb vermeide ich im Folgenden auch Termini wie “Kollostruktionsanalyse” oder “Kollexemanalyse”, sondern spreche allgemein von “Assoziationsmustern”. Die Kollostruktionsanalyse habe ich nur erwähnt, weil das besagte Paket eine praktische Möglichkeit bietet, auf einfachste Weise Assoziationsmuster zu errechnen, und der Name des Pakets andernfalls für Uneingeweihte etwas kryptisch bleiben würde. - Wenn Sie mehr darüber erfahren möchten, welche text- und diskurslinguistischen Aufschlüsse Assoziationsmuster im Allgemeinen erlauben, empfehle ich Bubenhofer (2009) oder auch viele der Blogeinträge, die über Bubenhofers Website verfügbar sind (https://www.bubenhofer.com/). Wir führen nun die Kollostruktionsanalyse durch und schauen uns die ersten paar Einträge an, die uns der head-Befehl zeigt: corona_coll &lt;- collex.dist(as.data.frame(corona_tbl)) # Ich stelle hier die Tabelle ohne die Spalte SHARED dar (die angibt, ob ein Lemma in beiden Datensätzen vorkommt), damit sie in eine Zeile passt: select(corona_coll, -SHARED) %&gt;% head(10) ## COLLEX O.CXN1 E.CXN1 O.CXN2 E.CXN2 ASSOC COLL.STR.LOGL SIGNIF ## 1 virus 32628 6623.4 237 26241.6 Freq 112753.565 ***** ## 2 krise 18019 3715.5 417 14720.5 Freq 56986.655 ***** ## 3 pandemie 6188 1247.1 0 4940.9 Freq 20168.335 ***** ## 4 viruspandemie 2221 447.6 0 1773.4 Freq 7158.780 ***** ## 5 infektion 1362 281.5 35 1115.5 Freq 4067.988 ***** ## 6 viruskrise 1076 216.9 0 859.1 Freq 3457.251 ***** ## 7 virusinfektion 1044 211.4 5 837.6 Freq 3292.928 ***** ## 8 patient 1027 228.7 108 906.3 Freq 2634.028 ***** ## 9 epidemie 759 156.4 17 619.6 Freq 2280.648 ***** ## 10 virusfall 675 136.0 0 539.0 Freq 2166.428 ***** Erwartungsgemäß führen -virus und -krise die Rangliste an, gefolgt von -pandemie und patient. Hier also keine Überraschungen. Interessant kann es auch sein, sich die letzten Ränge anzuschauen: select(corona_coll, -SHARED) %&gt;% tail(10) ## COLLEX O.CXN1 E.CXN1 O.CXN2 E.CXN2 ASSOC COLL.STR.LOGL SIGNIF ## 2754 weg 1 770.5 3822 3052.5 Freq_dwds21 1713.260 ***** ## 2755 preis 1 809.4 4015 3206.6 Freq_dwds21 1800.886 ***** ## 2756 daten 30 996.0 4912 3946.0 Freq_dwds21 1953.955 ***** ## 2757 version 1 924.0 4584 3661.0 Freq_dwds21 2059.496 ***** ## 2758 deutschland 13 986.5 4882 3908.5 Freq_dwds21 2072.096 ***** ## 2759 seite 3 954.7 4734 3782.3 Freq_dwds21 2102.843 ***** ## 2760 leben 2 964.9 4786 3823.1 Freq_dwds21 2138.409 ***** ## 2761 jahr 14 1066.5 5278 4225.5 Freq_dwds21 2241.806 ***** ## 2762 welt 11 1073.6 5316 4253.4 Freq_dwds21 2285.740 ***** ## 2763 programm 10 1259.2 6238 4988.8 Freq_dwds21 2712.821 ***** Wörter Deutschland, Seite, Leben, Welt kommen also relativ selten als Bestimmungsglied von Corona-Komposita vor. Wir haben nun auch die Möglichkeit, diese Assoziationswerte in unser oben erstelltes MotionChart zu integrieren, um so auch auf der x-Achse einen (halbwegs) bedeutungsvollen Wert zu zeigen. Weil die Assoziationswerte ebenfalls eine sehr schiefe Verteilung aufweisen, ist es sinnvoll, sie ebenfalls zu logarithmieren - das ist aber wirklich nur in diesem speziellen Fall sinnvoll, normalerweise würde man das eher nicht machen. Auch ist es so, dass die Assoziationen in unterschiedliche Richtungen gehen: Manche haben einen hohen LogLikelihood-Wert (das ist das Assoziationsmaß, das verwendet wurde), weil sie überzufällig häufig in den Corona-Daten auftauchen, andere haben einen hohen LogLikelihood-Wert, weil sie überzufällig häufig im Referenzkorpus auftauchen. Daher füge ich noch eine zusätzliche Spalte hinzu, in der ich diejenigen Werte, bei denen die Tendenz in Richtung Referenzkorpus (und damit weg vom Corona-Korpus) geht, negativiere. corona_coll$Log_assoc &lt;- log(corona_coll$COLL.STR.LOGL) corona_coll$Log_assoc2 &lt;- ifelse(corona_coll$ASSOC==&quot;Freq_dwds21&quot;, -corona_coll$Log_assoc, corona_coll$Log_assoc) Um die gewünschte Visualisierung erhalten zu können, müssen wir die Information zur Assoziationsstärke zur oben erstellten d2-Tabelle hinzufügen, mit der wir das MotionChart auf der vorigen Seite erstellt haben. Dafür nutzen wir wieder die left_join-Funktion. Mit dem by-Argument geben wir an, dass die Spalte mit den Daten, die in beiden Tabellen vorkommen und damit quasi als “Scharnier” für die Kombination beider Tabellen dienen, in der ersten Tabelle einen anderen Namen hat als in der zweiten; in d2 heißt sie nämlich “head” und in corona_coll “COLLEX”. Die Spaltennamen in der corona_coll-Tabelle sind alle die Default-Spaltennamen, die der Output der Funktionen im “collostructions”-Paket aufweist - nur falls Sie sich gewundert haben sollten, wo dieser Spaltenname herkommt. d2 &lt;- left_join(d2, corona_coll, by = c(&quot;head&quot; = &quot;COLLEX&quot;)) Die Daten können wir nun in bewährter Manier visualisieren. bubble2 &lt;- gvisMotionChart(d2, idvar = &quot;head&quot;, timevar = &quot;date&quot;, xvar = &quot;Log_assoc2&quot;, yvar = &quot;logFreq&quot;, sizevar=&quot;logFreq&quot;) plot(bubble2) Fig. 5.1: GoogleVis Motion Chart mit Frequenzen der Bestimmungsglieder, die im Coronakorpus mit dem Erstglied Corona- auftreten. Statt eines Schlussworts eher eine Zwischenbilanz: Ich hoffe, Sie konnten das eine oder andere aus diesem Tutorial lernen, auch wenn es noch work-in-progress ist und gerade diese dritte Seite noch etwas Feinschliff bedürfte, den ich hoffentlich bald ergänzen kann. Was aber deutlich geworden sein dürfte, ist, dass wir gerade aus der explorativen Auswertung der Daten interessante Trends ablesen können, auch wenn an der einen oder anderen Stelle Vorsicht geboten ist, gerade was die Genauigkeit der Daten angeht (Coronaauflagen erscheinen in den Daten zum Beispiel verdächtig früh, was darauf hindeutet, dass evtl. einige Daten nicht ganz stimmen). Anhand der Daten konnten wir uns zudem einige Grundbegriffe aus der korpusbasierten Erforschung morphologischer Phänomene erschließen, auch wenn wir mit den konkreten Kenngrößen kontextfrei nicht allzu viel anfangen könnten, sondern hierfür Vergleichswerte bräuchten, um interessante Tendenzen und Beobachtungen daraus ableiten zu können. "],
["literatur.html", "Literatur", " Literatur Baayen, R. Harald. 1993. On Frequency, Transparency, and Productivity. In Geert E. Booij &amp; Jaap van Marle (eds.), Yearbook of Morphology 1992, 181–208. Dordrecht: Kluwer. Baayen, R. Harald. 2009. Corpus Linguistics in Morphology: Morphological Productivity. In Anke Lüdeling &amp; Merja Kytö (eds.), Corpus Linguistics, 899–919. Berlin, New York: De Gruyter. Baayen, Harald &amp; Rochelle Lieber. 1991. Productivity and English Derivation: A Corpus-Based Study. Linguistics 29. 801–843. Barbaresi, Adrien. 2020. Coronakorpus. https://www.dwds.de/d/k-web#corona Bubenhofer, Noah. 2009. Sprachgebrauchsmuster. Korpuslinguistik als Methode der Diskurs- und Kulturanalyse. Berlin, Boston: de Gruyter. Gaeta, Livio &amp; Davide Ricca. 2006. Productivity in Italian word-formation. Linguistics 44(1). 57–89. Gries, Stefan Th. &amp; Anatol Stefanowitsch. 2004. Extending Collostructional Analysis: A Corpus-Based Perspective on “Alternations.” International Journal of Corpus Linguistics 9(1). 97–129. Hartmann, Stefan. 2016. Wortbildungswandel. Eine diachrone Studie zu deutschen Nominalisierungsmustern. Berlin, Boston: De Gruyter. Hartmann, Stefan. 2018. Derivational morphology in flux: a case study of word-formation change in German. Cognitive Linguistics 29(1). 77–119. Scherer, Carmen. 2006. Was ist Wortbildungswandel? Linguistische Berichte 205. 3–28. Stefanowitsch, Anatol. 2013. Collostructional Analysis. In Thomas Hoffmann &amp; Graeme Trousdale (eds.), The Oxford Handbook of Construction Grammar, 290–306. Oxford: Oxford University Press. "]
]
