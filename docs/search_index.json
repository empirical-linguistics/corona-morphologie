[
["index.html", "Corona-Morphologie: Einfache Produktivitätsanalysen an konkreten Beispielen 1 Einführung", " Corona-Morphologie: Einfache Produktivitätsanalysen an konkreten Beispielen Stefan Hartmann 2020-06-07 1 Einführung Eine zentrale Frage insbesondere in der Wortbildungsforschung, aber auch in anderen Bereichen wie etwa der Syntax, lautet, in welchem Maße ein konkretes Muster in der Lage ist, Neubildungen hervorzubringen. Beispielsweise bringt das Suffix -icht (Dickicht, Kehricht) synchron gar keine Neubildungen mehr hervor, während etwa -mäßig relativ viele Neubildungen wie etwa rezomäßig hervorbringt. In diesem Tutorial kann ich zwar nicht auf komplexere Aspekte der Produktivitätsmessung eingehen, möchte aber zumindest einen groben Überblick über verbreitete Maße der morphologischen Produktivität geben und anhand eines konkreten Beispiels zeigen, wie sie sich operationalisieren lassen. "],
["produktivitatsmae.html", "2 Produktivitätsmaße", " 2 Produktivitätsmaße Um die Produktivität sprachlicher Muster zu messen, bedient man sich i.d.R. dreier Größen, die auch in anderen Bereichen relevant sind. Für die gängigen Produktivitätsmaße benötigt man nämlich die Anzahl der Tokens die Anzahl der Types die Anzahl der Hapax Legomena. Was hat es damit auf sich? Tokens bezieht sich die Gesamtzahl der Wörter, die zu einem Wortbildungsmuster gehören. Angenommen, wir interessieren uns für Derivate auf -heit und -keit wie Freiheit und Ehrlichkeit und finden in einem winzig kleinen Korpus die Belege Freiheit, Freiheit, Freiheit, Ehrlichkeit, Faulheit und nochmal Faulheit, dann sind das insgesamt sechs Tokens, also sechs Vorkommnisse. Types bezeichnet die Anzahl der unterschiedlichen Instanzen eines Wortbildungsmusters. Im eben genannten Beispiel wären dies die drei Lexeme Freiheit, Ehrlichkeit und Faulheit. Hapax Legomena bezeichnet diejenigen Instanzen, die genau ein einziges Mal belegt sind. In unserem Beispiel wäre das Ehrlichkeit. Hapax Legomena, oder kurz Hapaxe, sind deshalb interessant, weil sie häufig als Index für Neubildungen gesehen werden: Zwar ist nicht davon auszugehen, dass jedes Wortbildungsprodukt, das nur einmal in einem Korpus belegt ist, eine ad-hoc-Bildung ist, denn auch eigentlich relativ häufige Wörter können in einem Korpus durch Zufall unterrepräsentiert sein. Man geht aber dennoch davon aus, dass es eine Korrelation zwischen Neubildungen und Hapaxen gibt, sodass Hapaxe als ungefährer Richtwert für die Zahl an Neubildungen gesehen werden können (die sich ja nicht messen lässt). An einem nicht-linguistischen Beispiel (das eine Idee von Susanne Flach aufgreift) illustriert 2.1 diese drei Konzepte. Gerade die Unterscheidung von Types und Tokens ist anhand solcher nicht-linguistischer Beispiele oft besser nachzuvollziehen. Zugleich zeigen solche Beispiele, dass das Konzept von Types immer von der Abstraktionsebene abhängt, auf der man sich gerade bewegt: Wenn es uns um die unterschiedlichen Arten von Cocktails geht, dann sehen wir dort drei Types - wenn es uns um die unterschiedlichen Arten von Getränken geht (“Cocktail” vs. “Bier” vs. “Limonade” etc.), dann sehen wir nur einen Type - nämlich Cocktails! Dementsprchend kann man auch bei der Arbeit mit linguistischen Daten Types oft auf unterschiedliche Art und Weise bestimmen (z.B. rein graphematisch, unter Zuhilfenahme von Grundformen [Lemmas] usw.). Fig. 2.1: Types und Tokens Mit Hilfe dieser drei Maße können wir uns nun den Produktivitätsmaßen annähern, die Baayen (u.a. 1993, 2009) in den 90er-Jahren vorgeschlagen hat und die seitdem immer wieder angewandt, aber auch kontrovers diskutiert wurden, da jedes dieser Maße nur einen Teilaspekt der morphologischen Produktivität erfassen kann und die Maße teilweise auch zu Fehlinterpretationen einladen, wenn man sich nicht ihrer Grenzen bewusst ist. Die realisierte Produktivität ist schlicht die Typefrequenz, die i.d.R. normalisiert berichtet wird, indem man die Anzahl der Tokens durch die Anzahl der Types teilt. Das wohl am häufigsten verwendete Maß ist die potentielle Produktivität. Sie ist der Quotient aus der Anzahl der Hapax Legomena, die zu einem Wortbildungsmuster gehören, und der Anzahl der Tokens im Korpus insgesamt. Dieses Maß wurde beispielsweise verwendet, um die Produktivität unterschiedlicher Wortbildungsmuster synchron zu vergleichen (Baayen &amp; Lieber 1991) oder um die Produktivität desselben Wortbildungsmusters in unterschiedlichen Zeitspannen zu vergleichen (Scherer 2006, Hartmann 2016). Dabei ist jedoch zu bedenken, dass der Wert der potentiellen Produktivität von der Korpusgröße und auch von der Tokenfrequenz abhängt (vgl. z.B. Gaeta &amp; Ricca 2006). Produktivitätswerte, die auf Grundlage unterschiedlich großer (Teil-)Korpora gewonnen wurden oder bei denen sich die Tokenfrequenz des Wortbildungsmusters zwischen den beiden (Teil-)Korpora unterscheiden, sind daher nicht unmittelbar vergleichbar - ein Problem, das leider oft nicht berücksichtigt wird (und das auch ich lange unterschätzt habe, vgl. Hartmann 2016; erst in Hartmann 2018 habe ich dem Rechnung getragen). Die expandierende Produktivität schließlich versucht zu messen, wie stark ein Wortbildungsmuster zum Wachstum des Wortschatzes einer Sprache beiträgt, indem sie die Anzahl der Hapaxe, die zum Wortbildungsmuster gehören, durch die Anzahl aller Hapaxe im Korpus teilt. Im Folgenden werden wir uns nur mit den ersten beiden Maßen auseinandersetzen und sie an einem konkreten Beispiel errechnen. "],
["corona-als-erstglied-eine-produktivitatsanalyse.html", "3 Corona- als Erstglied: Eine Produktivitätsanalyse Produktivitätsanalyse in Excel", " 3 Corona- als Erstglied: Eine Produktivitätsanalyse Als Beispiel sehen wir uns Komposita mit dem Erstglied Corona- an. Hier kann man zwar streiten, ob es sich um ein eigenes Wortbildungsmuster handelt oder ob wir es einfach mit Instanzen der allgemeineren N+N-Komposition zu tun haben, allerdings lässt sich durchaus argumentieren, dass Corona-Komposita in letzter Zeit so häufig geworden sind, dass man [Corona+X] als eigenständiges Kompositionsmuster annehmen kann. Dabei kann es nicht nur aus morphologischer, sondern auch aus sozio- und diskurslinguistischer Sicht interessant sein, einen Blick darauf zu werfen, mit welchen Bestimmungsgliedern sich das Erstglied Corona- verbindet. Praktischerweise können wir über das Digitale Wörterbuch der Deutschen Sprache (DWDS) mittlerweile auf ein Corona-Korpus zugreifen (Barbaresi 2020), das nach einmaliger Registrierung zugänglich ist. Aufgrund der flexibleren Suchanfragemöglichkeiten verwenden wir das Tool Dstar (https://kaskade.dwds.de/dstar/), um das Korpus zu durchsuchen. (Eine gute Anleitung zu Dstar findet sich in diesem Tutorial von Andreas Blombach: http://sprachwissenschaft.fau.de/personen/daten/blombach/korpora.pdf) Mit folgender Suchanfrage lasse ich mir alle Instanzen auszählen, bei denen auf die Buchstabenfolge Corona, ggf. gefolgt von einem Bindestrich, noch mindestens ein weiterer Buchstabe folgt: count( $w=/Corona-?.+/gi ) #by[$l]. (Das “gi” nach dem regulären Ausdruck ist eine Eigenheit der DWDS-Suchanfragesprache DDC: Mit g gebe ich an, dass ich genau das suche, also bspw. nichts, wo dem String Corona noch Anderes vorausgeht, und mit i gebe ich an, dass die Suche case-insensitive sein soll, also Groß- und Kleinschreibung ignoriert werden sollen). Fig. 3.1: Types und Tokens Wie 3.1 zeigt, habe ich als Exportoption “Text” gewählt, sodass wir die Daten im Rohtextformat erhalten, und die Seitengröße (“Page size”) so weit erhöht, dass auf jeden Fall alle Types ausgegeben werden (der Default wäre hier bei weitem nicht ausreichend). So erhalten wir eine Tabelle mit einzelnen Types und deren jeweiliger Frequenz. Mit dieser arbeiten wir nun weiter. Produktivitätsanalyse in Excel Die Tabelle können wir z.B. in Excel copy&amp;pasten (dabei können Sie sich ggf. an folgendem Tutorial orientieren: https://empirical-linguistics.github.io/korpus-schnelleinstieg/von-der-fragestellung-zur-konkordanz.html#import-in-excel) In 3.2 habe ich die Tabelle zudem noch mit Überschriften versehen, die in der von Dstar erzeugten Datei zunächst nicht dabei sind, und habe sie als Tabelle formatiert (Einfügen &gt; Tabelle, vgl. wiederum das oben verlinkte Tutorial). Fig. 3.2: Types und Tokens Es ist nun denkbar einfach, die Zahl der Types, Tokens und Hapax Legomena zu errechnen. Wenn man es sich ganz einfach machen möchte, kann man die Types ermittelt, indem man schaut, wie viele Zeilen die Tabelle hat (dabei muss man die Kopfzeile abziehen) und die Anzahl der Tokens, indem man mit Excels AutoSum-Funktion die Frequenzen in der linken Spalte aufsummiert. Die Hapax Legomena kann man zählen, indem man die Daten nach Frequenz ordnet und dann mit Hilfe der Zeilennummerierung errechnet, wie viele Types genau einmal belegt sind. Man kann es aber auch etwas professioneller machen, indem man die Frequenzwerte in der “Freq”-Spalte mit Hilfe einer PivotTable auszählt. Dafür klickt man auf Einfügen &gt; PivotTable und setzt im sich öffnenden neuen Fenster ein Häkchen bei “Freq”. Defaultmäßig zeigt Excel nun die Summe von “Freq”. Dieser Wert ist die Gesamtzahl der Tokens: 91.606. Um stattdessen die Zahl der Types und Hapaxe zu bekommen, ändern wir im “Werte”-Fenster unten rechts die Funktion, die die Daten auswertet, von “Summe” zu “Zählen” (engl. count). Nun sehen wir zunächst die Anzahl der Types (3976). Wenn wir nun “Freq” aus dem Fenster oben ins Fenster “Zeilen” unten links ziehen, dann wird das Ganze nach den einzelnen Frequenzwerten aufgeschlüsselt: Nun sehen wir also, wie viele Types 1-mal belegt sind, wie viele 2-mal usw. Daraus können wir die Anzahl der Hapax Legomena ablesen: 2313. Wir haben nun also alle Werte, die wir brauchen: Anzahl der Tokens: 91.606 Anzahl der Types: 3976 Anzahl der Hapax Legomena: 2313 Theoretisch könnte man in Excel nun direkt die einzelnen Werte berechnen. Um in Excel etwas zu berechnen, muss man ein Gleichheitszeichen davor setzen; die Formeln wären also: Potentielle Produktivität: Anzahl der Hapaxe / Anzahl der Tokens: = 2313 / 91606 Realisierte Produktivität: Zahl der Types / Zahl der Tokens: = 3976 / 91606 Da wir die Anzahl der Hapaxe nicht kennen, verzichten wir darauf, die expandierende Produktivität zu ermitteln. Allerdings ließe sich die Gesamtzahl der Hapax Legomena im Coronakorpus relativ einfach über Dstar herausfinden, indem man mit count(* #sep) #by[$w] alle Tokens im Coronakorpus auszählen lässt und dann die Hapaxe so auszählt, wie wir es eben getan haben - allerdings ist die Datenmenge sehr viel größer, sodass diese Aufgabe einfacher zu erledigen ist, wenn man über basale Programmierkenntnisse verfügt und eine Programmiersprache wie R oder Python diese Aufgabe erledigen lassen kann. Bevor wir weitermachen, müssen wir uns noch kurz der Frage widmen, was uns die hier errechneten Produktivitätsmaße eigentlich sagen. Die nüchterne Antwort ist: Zunächst einmal nichts. Solche Werte machen nur im Kontext Sinn - also wenn man beispielsweise, wie es die oben angeführte Forschungsliteratur tut, verschiedene Wortbildungsmuster miteinander vergleicht oder die Produktivitätsentwicklung diachron verfolgt (mit dem erwähnten Caveat, dass man eigentlich gleich große Datenmengen bräuchte). Im Falle unseres Beispiels könnte man bspw. die Produktivität des Musters 2019, 2020 und 2021 vergleichen (natürlich erst, wenn man die Daten dafür hat - ich schreibe dies Mitte 2020), oder man könnte die Produktivität von Corona-Komposita mit anderen Kompositionsmustern vergleichen, die sich auf ähnlich krisenhafte Ereignisse beziehen. Fig. 3.3: Zahl der Types und Hapaxe mit Hillfe von PivotTables in Excel ermitteln "],
["morphologische-produktivitat-live-ein-blick-auf-die-bestimmungsglieder.html", "4 Morphologische Produktivität live: Ein Blick auf die Bestimmungsglieder", " 4 Morphologische Produktivität live: Ein Blick auf die Bestimmungsglieder In diesem Abschnitt wollen wir uns nun einen ganz anderen Weg anschauen, auf dem wir uns den Daten nähern können. Hierfür arbeiten wir mit R - falls Sie sich hier einarbeiten wollen, finden Sie online extrem viele gute Tutorials, und es gibt auch viele Einführungen in die Statistik speziell für Linguist*innen, die mit R arbeiten. Hier kann ich nicht in R einführen, versuche aber, das, was ich mache, so verständlich zu beschreiben, dass Sie zumindest das Konzept hinter dem, was ich mache, verstehen können. Zunächst lade ich einige Zusatzpakete, die ich im Folgenden benutzen möchte. Sie enthalten Funktionen, die in R nicht standardmäßig enthalten sind. Wenn Sie die Pakete noch nicht installiert haben, müssen Sie siejedes einzelne zunächst mit install.packages(&quot;Paketname&quot;) installieren. Das gilt für diejenigen, die über das “Comprehensive R Architecture Network” (CRAN) verfügbar sind. Bei einem der Pakete, “collostructions”, ist dies nicht der Fall, Sie finden es unter www.sfla.ch. library(tidyverse) ## ── Attaching packages ───────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ ggplot2 3.2.1 ✓ purrr 0.3.4 ## ✓ tibble 3.0.0 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.0 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.4.0 ## Warning: package &#39;tibble&#39; was built under R version 3.6.2 ## Warning: package &#39;purrr&#39; was built under R version 3.6.2 ## ── Conflicts ──────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(googleVis) ## Creating a generic function for &#39;toJSON&#39; from package &#39;jsonlite&#39; in package &#39;googleVis&#39; ## ## Welcome to googleVis version 0.6.4 ## ## Please read Google&#39;s Terms of Use ## before you start using the package: ## https://developers.google.com/terms/ ## ## Note, the plot method of googleVis will by default use ## the standard browser to display its output. ## ## See the googleVis package vignettes for more details, ## or visit https://github.com/mages/googleVis. ## ## To suppress this message use: ## suppressPackageStartupMessages(library(googleVis)) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date library(collostructions) Zunächst lesen wir die Daten ein, werfen einen Blick auf die Struktur der Daten und schauen uns diese mit Hilfe des str()-Befehls genauer an. d &lt;- read_delim(&quot;Coronakomposita.txt&quot;, delim = &quot;\\t&quot;, col_names = c(&quot;Freq&quot;, &quot;word&quot;, &quot;date&quot;)) ## Parsed with column specification: ## cols( ## Freq = col_double(), ## word = col_character(), ## date = col_date(format = &quot;&quot;) ## ) d ## # A tibble: 14,682 x 3 ## Freq word date ## &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 Corona 2020-03-04 ## 2 1 Corona* 2020-04-01 ## 3 1 Corona* 2020-04-03 ## 4 1 Corona* 2020-04-08 ## 5 1 Corona* 2020-04-09 ## 6 1 Corona* 2020-04-13 ## 7 1 Corona* 2020-04-18 ## 8 1 Corona* 2020-05-08 ## 9 1 Corona* 2020-05-18 ## 10 1 Corona* 2020-05-22 ## # … with 14,672 more rows str(d) ## tibble [14,682 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Freq: num [1:14682] 1 1 1 1 1 1 1 1 1 1 ... ## $ word: chr [1:14682] &quot;Corona&quot; &quot;Corona*&quot; &quot;Corona*&quot; &quot;Corona*&quot; ... ## $ date: Date[1:14682], format: &quot;2020-03-04&quot; &quot;2020-04-01&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Freq = col_double(), ## .. word = col_character(), ## .. date = col_date(format = &quot;&quot;) ## .. ) Nun löschen wir alle Datenpunkte, in denen “Corona” nicht von mindestens zwei weiteren Zeichen gefolgt wird, um Fehltreffer wie Corona- zu tilgen. Dann konvertieren wir alle Daten in Kleinschreibung und fügen eine Spalte hinzu, die nur die Zweitglieder enthält (also alles, was auf Corona- folgt). Aus dieser Spalte löschen wir zusätzlich noch alle Interpunktionszeichen, um z.B. Fälle der Kompositaschreibung mit und ohne Bindestrich zu vereinheitlichen. # Fehltreffer reduzieren d &lt;- d[-which(sapply(1:nrow(d), function(i) nchar(d$word[i]))&lt;=8),] # Groß- und Kleinschreibung ignorieren d$word &lt;- tolower(d$word) # Köpfe d$head &lt;- gsub(&quot;corona-?&quot;, &quot;&quot;, d$word) # Interpunktion löschen d$head &lt;- gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, d$head) Wir fassen die Daten nun mit der summarise-Funktion aus dem dplyr-Paket (Teil des “Tidyverse”) zusammen, um die Gesamtfrequenzen jedes Bestimmungsglieds zu bekommen, und ordnen die Daten absteigend nach Frequenz. Das Ganze speichern wir in einen eigene Dataframe, den wir corona_tbl nennen. corona_tbl &lt;- d %&gt;% group_by(head) %&gt;% summarise( Freq = sum(Freq) ) %&gt;% arrange(desc(Freq)) Wir möchten nun wissen, welche Lexeme überzufällig häufig als Bestimmungsglieder in Corona-Komposita auftreten. Dafür brauchen wir zunächst ein Referenzkorpus. Hierfür habe ich eine Frequenzliste aller als Nomen getaggten Tokens aus dem DWDS-Kernkorpus des 21. Jahrhunderts erstellt (wiederum mit Dstar), die wir nun einlesen. Wiederum konvertieren wir alle Daten in Kleinschreibung und zählen die Frequenz entsprechend aus: # einlesen nouns &lt;- read_delim(&quot;allnouns_dwds21.txt&quot;, delim = &quot;\\t&quot;, quote=&quot;&quot;, col_names = c(&quot;Freq_dwds21&quot;, &quot;word&quot;)) ## Parsed with column specification: ## cols( ## Freq_dwds21 = col_double(), ## word = col_character() ## ) # alle in Kleinschreibung nouns$word &lt;- tolower(nouns$word) # neu auszählen (um groß- und kleingeschriebene Varianten zu vereinen) nouns &lt;- nouns %&gt;% group_by(word) %&gt;% summarise( Freq_dwds21 = sum(Freq_dwds21) ) Nun verbinden wir mit Hilfe der left_join-Funktion die oben erstellte corona_tbl-Tabelle mit der soeben erstellten nouns-Tabelle. Da nicht alle Lexeme, die in der Corona-Tabelle belegt sind, auch in der DWDS21-Tabelle belegt sind, gibt es einige fehlende Datenpunkte (in R heißen diese NA, für “Not Available”). Diese ersetzen wir mit Hilfe des replace_na-Befehls durch 0. corona_tbl &lt;- left_join(corona_tbl, nouns, by = c(&quot;head&quot; = &quot;word&quot;)) corona_tbl &lt;- replace_na(corona_tbl, list(Freq_dwds21 = 0)) Mit Hilfe der Funktion collex.dist aus dem collostructions-Paket können wir nun eine sog. distinktive Kollexemanalyse über die Daten laufen lassen. Wenn Sie mehr über dieses Verfahren lesen möchten, können Sie z.B. das Paper von Gries &amp; Stefanowitsch (2004) oder den Überblicksartikel zur Kollostruktionsanalyse von Stefanowitsch (2013) lesen. Beachten Sie aber bitte, dass ich dieses Verfahren hier überhaupt nicht im Sinne der Erfinder verwende, sondern lediglich explorativ auf Datensätze anwende. Wenn Sie mehr darüber erfahren möchten, welche text- und diskurslinguistischen Aufschlüsse solche Assoziationsmuster erlauben, empfehle ich Bubenhofer (2009) oder auch viele der Blogeinträge, die über Bubenhofers Website verfügbar sind (https://www.bubenhofer.com/). Wir führen nun die Kollostruktionsanalyse durch und schauen uns die ersten paar Einträge an, die uns der head-Befehl zeigt: corona_coll &lt;- collex.dist(as.data.frame(corona_tbl)) head(corona_coll) ## COLLEX O.CXN1 E.CXN1 O.CXN2 E.CXN2 ASSOC COLL.STR.LOGL SIGNIF SHARED ## 1 virus 32628 6623.4 237 26241.6 Freq 112753.565 ***** Y ## 2 krise 18019 3715.5 417 14720.5 Freq 56986.655 ***** Y ## 3 pandemie 6188 1247.1 0 4940.9 Freq 20168.335 ***** N ## 4 viruspandemie 2221 447.6 0 1773.4 Freq 7158.780 ***** N ## 5 infektion 1362 281.5 35 1115.5 Freq 4067.988 ***** Y ## 6 viruskrise 1076 216.9 0 859.1 Freq 3457.251 ***** N Erwartungsgemäß führen -virus und -krise die Rangliste an. Nun wollen wir aber mit diesen Daten noch mehr machen und einen Plot erstellen, in dem wir sowohl die Frequenz als auch die Assoziationsstärke sehen. Dafür ist es sinnvoll, diese Daten zunächst zu transformieren. Gerade bei Frequenzdaten ist es üblich, sie zu logarithmieren, da Wortfrequenzen oft einer sehr schiefen Verteilung folgen, wie wir auch an diesen Daten sehen können: plot(corona_tbl$Freq, type = &quot;b&quot;, ylab = &quot;Absolute Frequenz&quot;) Einige wenige Tokens sind extremst häufig, viele sehr selten und die allermeisten Hapaxe, haben also eine Frequenz von 1. Anders sieht es aus, wenn wir die Daten logarithmieren: plot(log(corona_tbl$Freq), type = &quot;b&quot;, ylab = &quot;Logarithmierte Frequenz&quot;) Die Verteilung ist nun schon deutlich weniger schief. # Log-Frequenz hinzufügen corona_coll$LogFreq &lt;- log(corona_coll$O.CXN1) Um die Entstehung neuer Wörter über die Zeit hinweg visualisieren zu können, müssen wir aber zunächst die Assoziationswerte dem ursprünglichen Dataframe, der ja auch die Datumswerte hat, hinzufügen, was wir wieder über den left_join-Befehl tun. Anschließend tabulieren wir die Daten so, dass wir für jedes Datum die Frequenz bekommen. d &lt;- left_join(d, corona_coll, by = c(&quot;head&quot; = &quot;COLLEX&quot;), all.x = T) d$LogFreq_per_date &lt;- log(d$Freq) # tabulieren: d2 &lt;- d %&gt;% group_by(head, date) %&gt;% summarise( Freq = sum(Freq), Freq_in_entire_dataset = unique(O.CXN1) ) Um die Datenmenge handhabbar zu halten, beschränken wir uns zudem auf solche Daten, deren absolute Frequenz in den Daten mindestens fünf beträgt und die im Jahr 2020 erhoben wurden. Dafür fügen wir zunächst noch eine Jahr-Spalte an. # Jahr-Spalte d2$year &lt;- gsub(&quot;-.*&quot;, &quot;&quot;, d2$date) # nur 2020 d2 &lt;- filter(d2, year == &quot;2020&quot;) # Frequenz &gt; 5 d2 &lt;- filter(d2, Freq_in_entire_dataset &gt;= 5) Um die Daten als Input für Googlevis benutzen zu können, müssen wir sie aber zunächst noch in ein anderes, tabellarisches Format bringen. # tabellarisches Format d2_tbl &lt;- d2 %&gt;% select(head, date) %&gt;% table %&gt;% as.data.frame(stringsAsFactors = F) d2_tbl$date &lt;- d2_tbl$date %&gt;% as_date() colnames(d2_tbl) &lt;- c(&quot;head&quot;, &quot;date&quot;, &quot;Freq_on_date&quot;) d2_tbl2 &lt;- left_join(d2_tbl, d2, by = c(&quot;head&quot;, &quot;date&quot;)) d2_tbl3 &lt;- replace_na(d2_tbl2, list(Freq = 0, logFreq = 0, diff_rel = 0)) # mit Kollexemtabelle vereinigen d2_tbl4 &lt;- left_join(d2_tbl3, corona_coll, by = c(&quot;head&quot; = &quot;COLLEX&quot;)) Im Falle der hier vorliegenden Daten macht es auch bei den Log-Likelihood-Werten, die in unserer Kollostruktionstabelle als Assoziationsmaß verwendet werden, Sinn, sie zu logarithmieren, weil man sonst in unserer Grafik wenig erkennen würde. Das ist aber wirklich nur in diesem speziellen Fall zwecks Visualisierung sinnvoll und in den meisten Fällen unnötig. Darüber hinaus nehme ich noch eine weitere Veränderung vor: Der Log-Likelihood-Wert geht in zwei Richtungen - einige haben einen hohen LogL-Wert, weil sie überzufällig häufig als Bestimmungsglieder von Komposita im Coronakorpus vorkommen, andere haben einen hohen LogL-Wert, weil sie überzufällig häufig als freie Lexeme im DWDS-Kernkorpus 21 vorkommen! Die ASSOC-Spalte im collex.dist-Output gibt an, in welche Richtung die Assoziation jeweils geht. Mit Hilfe dieser Spalte negativiere ich alle Werte, die in Richtung DWDS21 gehen. So können wir dann in unserer Visualisierung auf Anhieb sehen, in welche Richtung die Assoziation geht. # Frequenz logarithmieren d2_tbl4$logFreq &lt;- log(d2_tbl4$Freq_on_date) # Kollexemstärke logarithmieren d2_tbl4$collex &lt;- ifelse(d2_tbl4$ASSOC==&quot;Freq_dwds21&quot;, -d2_tbl4$COLL.STR.LOGL, d2_tbl4$COLL.STR.LOGL) d2_tbl4$collex2 &lt;- ifelse(d2_tbl4$ASSOC==&quot;Freq_dwds21&quot;, -log1p(d2_tbl4$COLL.STR.LOGL), log1p(d2_tbl4$COLL.STR.LOGL)) # GoogleMotionChart erstellen... bubble &lt;- gvisMotionChart(filter(select(d2_tbl4, head, date, logFreq, COLL.STR.LOGL, collex, collex2), logFreq &gt; 0), idvar = &quot;head&quot;, sizevar = &quot;logFreq&quot;, yvar=&quot;logFreq&quot;, xvar=&quot;collex2&quot;, timevar = &quot;date&quot;) # ... und mit dem Plot-Befehl aktivieren plot(bubble) ## starting httpd help server ... done Et voilà - wenn Sie es schaffen, den FlashPlayer zu aktivieren, können Sie jetzt ein wenig mit den Daten herumspielen. Statt eines Schlussworts eher eine Zwischenbilanz: Ich hoffe, Sie konnten das eine oder andere aus diesem Tutorial lernen, auch wenn es noch work-in-progress ist und gerade diese dritte Seite noch etwas Feinschliff bedürfte. Ich hoffe, dass ich diesen Feinschliff bald ergänzen kann… "],
["literatur.html", "Literatur", " Literatur Baayen, R. Harald. 1993. On Frequency, Transparency, and Productivity. In Geert E. Booij &amp; Jaap van Marle (eds.), Yearbook of Morphology 1992, 181–208. Dordrecht: Kluwer. Baayen, R. Harald. 2009. Corpus Linguistics in Morphology: Morphological Productivity. In Anke Lüdeling &amp; Merja Kytö (eds.), Corpus Linguistics, 899–919. Berlin, New York: De Gruyter. Baayen, Harald &amp; Rochelle Lieber. 1991. Productivity and English Derivation: A Corpus-Based Study. Linguistics 29. 801–843. Barbaresi, Adrien. 2020. Coronakorpus. https://www.dwds.de/d/k-web#corona Bubenhofer, Noah. 2009. Sprachgebrauchsmuster. Korpuslinguistik als Methode der Diskurs- und Kulturanalyse. Berlin, Boston: de Gruyter. Gaeta, Livio &amp; Davide Ricca. 2006. Productivity in Italian word-formation. Linguistics 44(1). 57–89. Gries, Stefan Th. &amp; Anatol Stefanowitsch. 2004. Extending Collostructional Analysis: A Corpus-Based Perspective on “Alternations.” International Journal of Corpus Linguistics 9(1). 97–129. Hartmann, Stefan. 2016. Wortbildungswandel. Eine diachrone Studie zu deutschen Nominalisierungsmustern. Berlin, Boston: De Gruyter. Hartmann, Stefan. 2018. Derivational morphology in flux: a case study of word-formation change in German. Cognitive Linguistics 29(1). 77–119. Scherer, Carmen. 2006. Was ist Wortbildungswandel? Linguistische Berichte 205. 3–28. "]
]
